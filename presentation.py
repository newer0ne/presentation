from asyncore import write
import streamlit as st
from gsheetsdb import connect
import pandas as pd
import io
import requests
from io import StringIO

conn = connect()
@st.cache(ttl=1200)
def run_query(query):
    rows = conn.execute(query, headers=1)
    return rows

st.markdown("<h2 style='text-align: center;'>Datalyzer</h2>", unsafe_allow_html=True)

headcol1, headcol2 = st.columns(2)

with headcol1:
    header_tasklink = """[<h5 style='text-align: center;'>Test task:</h5>](https://xoservices.notion.site/1872d331265946a0ae2c5c9069189fd7)"""
    st.markdown(header_tasklink, unsafe_allow_html=True)

with headcol2:
    header_repolink = """[<h5 style='text-align: center;'>Github repo this project:</h5>](https://github.com/newer0ne/presentation/blob/main/presentation.py)"""
    st.markdown(header_repolink, unsafe_allow_html=True)

class Dataset:
    def __init__(self) -> None:
        self.name = []  
        self.df = []
        self.dfi = []
        self.listcols = []
        self.link = []
        self.buffer = io.StringIO()
        self.up = None
    
    def Open(self, index):
        self.df = pd.read_csv(index)
        self.name = index
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])

    def linkup(self, index):
        file_id = self.link.split('/')[-2]
        dwn_url = 'https://drive.google.com/uc?export=download&id=' + file_id
        url2 = requests.get(dwn_url).text
        csv_raw = StringIO(url2)
        self.df = pd.read_csv(csv_raw)
        self.name = index
        st.dataframe(self.df)
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        st.text(self.listcols)

    def upload(self, index):
        self.up = st.file_uploader("CSV file upload area, for example " + index)
        if self.up is not None:
            st.write("File name: ", self.up.name)
            self.df = pd.read_csv(self.up)
            st.dataframe(self.df)
            self.name = self.up.name
            for i in range(len(self.df.columns)):
                self.listcols.append(self.df.columns[i])
            st.text(self.listcols)

    def DFinfo (self):
        st.write('–û–±–∑–æ—Ä  –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞', self.name, ':')
        st.dataframe(self.df)
        st.write('–û–±–∑–æ—Ä  —Å–≤–µ–¥–µ–Ω–∏–π –æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ ', self.name, ':')
        self.dfi = self.df
        self.dfi.info(buf = self.buffer)
        self.dfi = self.buffer.getvalue()
        st.text(self.dfi)

    def renamecol(self, oldname, newname):
        self.df[newname] = self.df[oldname]
        del self.df[oldname]
        self.listcols.remove(oldname)
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        st.text(self.listcols)
        st.dataframe(self.df)

opt_desc = ["–û—Ç–∫—Ä—ã—Ç—å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "–û—Ç–∫—Ä—ã—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–µ", "–ó–∞–≥—Ä—É–∑–∏—Ç—å –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö"]
load_option = st.radio(
    "–í—ã–±–æ—Ä —Å–ø–æ—Å–æ–±–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:",
    (opt_desc))

name_list = ["ads.csv", "leads.csv", "purchases.csv"]
data1 = Dataset()
data1.link = st.secrets["ads"]
data2 = Dataset()
data2.link = st.secrets["leads"]
data3 = Dataset()
data3.link = st.secrets["purchases"]
data4 = Dataset()


st.write('–ù–∏–∂–µ –º–æ–∂–Ω–æ –æ—Å–º–æ—Ç—Ä–µ—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö:')
if load_option == opt_desc[0]:    
    tab_open1, tab_open2, tab_open3 = st.tabs(name_list)

    with tab_open1:
        data1.Open(name_list[0])
        data1.DFinfo()

        # –ü—Ä–∏–≥–æ–¥–∏—Ç—Å—è –ø–æ–¥ —Ü–∏–∫–ª —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö
        for i in range(len(data2.listcols)):
            print(i)
            #data2.df.iloc[:, i].unique()
            #i += 1
        
            #data2.listcols = st.columns(len(data2.listcols))
            #for i in data2.listcols[i]:
            #   data2.listcols[i].write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 1: ")
            #    data2.listcols[0] = 1
#
#        self.colop1.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 1: ")
#        self.colop1.dataframe(self.df.iloc[:, 0].unique())
#        self.colop2.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 2: ")
#        self.colop2.dataframe(self.df.iloc[:, 1].unique())
#        self.colop3.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 3: ")
#        self.colop3.dataframe(self.df.iloc[:, 2].unique())
#        self.colop4.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 4: ")
#        self.colop4.dataframe(self.df.iloc[:, 3].unique())
#        self.colop5.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 5: ")
#        self.colop5.dataframe(self.df.iloc[:, 4].unique())
#        self.colop6.write("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ 6: ")
#        self.colop6.dataframe(self.df.iloc[:, 5].unique())

        st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å—É–¥—è –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É,
        –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–∞–ª–∞–º–µ ('utm_medium' = 'cpc') 
        –≤ —è–Ω–¥–µ–∫—Å.–¥–∏—Ä–µ–∫—Ç ('utm_source' = 'yandex').""")
        st.write("""–°—Ç–æ–ª–±—Ü—ã 'd_ad_account_id' –∏ 'd_utm_term' –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        –Ω–µ –Ω–µ—Å—É—Ç —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–¥–∞–ª—è—é—Ç—Å—è, –ø–æ—Å–∫–æ–ª—å–∫—É 'd_ad_account_id' –∏–º–µ–µ—Ç 
        —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ 'xo-for-client-ya', –∞ 'd_utm_term' –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç–æ–π.""")
        st.write("""–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ 'm_clicks' –∏ 'm_cost' –≤ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ,
        –∞ —Ç–∞–∫-–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ 'm_clicks' –±–æ–ª—å—à–µ –Ω—É–ª—è.""")

        data1.df = data1.df.drop(columns = ['d_ad_account_id', 'd_utm_term'])
        data1.df['m_cost'] = data1.df['m_cost'].astype(int)
        data1.df['m_clicks'] = data1.df['m_clicks'].astype(int)
        data1.df = data1.df[data1.df['m_clicks'] > 0]
        data1.DFinfo()

    with tab_open2:
        data2.Open(name_list[1])
        data2.DFinfo()

        st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å—É–¥—è –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É,
        –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –∑–∞—è–≤–∫–∞–º, —Å–æ–∑–¥–∞–Ω–Ω—ã–º –Ω–∞ —Å–∞–π—Ç–µ.""")
        st.write("""–°—Ç–æ–ª–±—Ü—ã 'd_ad_account_id' –∏ 'd_utm_term' –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        –Ω–µ –Ω–µ—Å—É—Ç —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–¥–∞–ª—è—é—Ç—Å—è, –ø–æ—Å–∫–æ–ª—å–∫—É 'd_ad_account_id' –∏–º–µ–µ—Ç 
        —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ 'xo-for-client-ya', –∞ 'd_utm_term' –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç–æ–π.""")

        data2.df = data2.df[(data2.df['d_lead_utm_source'] == 'yandex') & (data2.df['d_lead_utm_medium'] == 'cpc')]
        data2.df['client_id'] = data2.df['client_id'].astype(str)
        data2.df = data2.df[(data2.df['client_id'] != 'nan')]
        data2.df = data2.df[(data2.df['d_lead_utm_content'].notnull())]
        data2.df = data2.df.drop(columns = ['d_lead_utm_term'])
        data2.DFinfo()

    with tab_open3:
        data3.Open(name_list[2])
        data3.DFinfo()    

        data3.df['client_id'] = data3.df['client_id'].astype(str)
        data3.df = data3.df[(data3.df['client_id'] != 'nan')]
        data3.df['m_purchase_amount'] = data3.df['m_purchase_amount'].astype(int)
        data3.df = data3.df[(data3.df['m_purchase_amount'] > 0)]
        st.dataframe(data3.df)

elif load_option == opt_desc[1]:
    tab_load1, tab_load2, tab_load3 = st.tabs(name_list)    
    with tab_load1:
        data1.linkup(name_list[0])    
    with tab_load2:
        data2.linkup(name_list[1])        
    with tab_load3:
        data3.linkup(name_list[2])

elif load_option == opt_desc[2]:
    tab_up1, tab_up2, tab_up3 = st.tabs(name_list)
    with tab_up1:
            data1.upload(name_list[0])
    with tab_up2:
            data2.upload(name_list[1])
    with tab_up3:
            data3.upload(name_list[2])

if (data1.df is None) or (data2.df is None) or (data3.df is None):
    st.error('This is an error', icon="üö®")
    pause()

m23 = pd.merge(data2.df, data3.df, how = 'left', on = 'client_id')

st.write("–§–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö astype(int), 'utm_source' == 'yandex' –∏ 'm_purchase_amount'] > 0:")

m23f = m23[(m23['utm_source'] == 'yandex') & (m23['m_purchase_amount'] > 0)]
m23f['m_purchase_amount'] = m23f['m_purchase_amount'].astype(int)
m23f['utm_content'] = m23f['utm_content'].astype(int)
m23f['utm_campaign'] = m23f['utm_campaign'].astype(int)

st.write('')
st.dataframe(m23f)

st.write("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö merget tables –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")

buffer4 = io.StringIO()
m23i = m23f
m23i.info(buf = buffer4)
m23i = buffer4.getvalue()
st.text(m23i)

st.write("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è merge ads + leads_purchase:")

data1.df.astype({'m_clicks': 'int'})
buffer5 = io.StringIO()
data1.dfi = data1.df
data1.dfi.info(buf = buffer5)
data1.dfi = buffer5.getvalue()
st.text(data1.dfi)

m123 = pd.merge(data1.df, m23f, how = 'left', on = ['created_at', 'utm_medium','utm_source', 'utm_campaign', 'utm_content'])
#m123 = m123[(m123.m_purchase_amount > 0)]
#m123['m_purchase_amount'] = m123['m_purchase_amount'].astype(int)

st.text("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö merget tables ads_leads_purchase:")

buffer6 = io.StringIO()
m123i = m123
m123i.info(buf = buffer6)
m123i = buffer6.getvalue()
st.text(m123i)

st.text("–£–¥–∞–ª—è–µ–º —Å—Ç–æ–±–µ—Ü 'utm_content', 'purchase_id' –∏ 'client_id':")

m123 = m123.drop(columns = ['utm_content', 'purchase_id', 'client_id'])
st.dataframe(m123)

st.text('–û–ø—Ä–µ–¥–µ–ª–∏–º —Å—Ç—Ä–æ–∫–∏ —Å —Ä–∞–∑–Ω–∏—Ü–µ–π –ø–æ –æ–ø–ª–∞—Ç–∞–º –≤ 15 –¥–Ω–µ–π:')

m123['created_at'] = m123['created_at'].astype(str)
m123['DATE'] = pd.to_datetime(m123['created_at'], infer_datetime_format=True)  
m123['DATE'] = pd.to_datetime(m123['DATE'], format = "%y-%m-%d")
m123['DATE_P'] = pd.to_datetime(m123['purchase_created_at'], infer_datetime_format=True)  
m123['DATE_P'] = pd.to_datetime(m123['DATE_P'], format = "%y-%m-%d")
m123['Difference'] = (m123['DATE_P'] - m123['DATE']).dt.days
m123 = m123.drop(columns = ['DATE', 'DATE_P'])
m123 = m123[(m123.Difference >= 0)]

buffer7 = io.StringIO()
m123ii = m123
m123ii.info(buf = buffer7)
m123ii = buffer7.getvalue()

st.text('–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∞–±–ª–∏—Ü—ã –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –¥–∞—Ç—ã')
st.dataframe(m123)
st.text(m123ii)

#m123['–î–∞—Ç–∞_–æ–ø–ª–∞—Ç—ã'] = datetime.strptime(m123['purchase_created_at'], '%m-%d-%Y').date()
#m123['Difference'] = (m123['–î–∞—Ç–∞'] - m123['–î–∞—Ç–∞_–æ–ø–ª–∞—Ç—ã']).dt.days
#st.dataframe(m123)


#buffer = io.StringIO()
#joined123.info(buf = buffer)
#joined_df_info = buffer.getvalue()
#st.text(joined_df_info)

#df_to_download = joined123.to_csv()
#st.download_button(label='üì• Download .CSV', data = df_to_download, file_name = "Joined dataframe" + ".csv")



#st.write("Dataframe Renamer")
#ren1, ren2, ren3 = st.columns(3)

#with ren1:
#    data4.name = st.radio(
#        "Dataframe selection to rename üëâ",
#        (data1.name, data2.name, data3.name))
#    st.write("Choosed dataframe: " + data4.name)

#with ren2:
#    if data4.name == data1.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data1.listcols))
#        st.write("Choosed column: " + ren_col)
#    elif data4.name == data2.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data2.listcols))
#        st.write("Choosed column: " + ren_col)
#    elif data4.name == data3.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data3.listcols))
#        st.write("Choosed column: " + ren_col)

#with ren3:
#    newcolname = st.text_input('New column name', ren_col)
#    st.write("New name for the selected column: " + newcolname)

#if st.button('Lets change it'):
#    if data4.name == name_list[0]:
#        data1.renamecol(ren_col, newcolname)
#    elif data4.name == name_list[1]:
#        data2.renamecol(ren_col, newcolname)
#    elif data4.name == name_list[2]:
#        data3.renamecol(ren_col, newcolname)


#with st.expander("Dataset Joiner"):
#    col4, col5, col6, col7 = st.columns(4)
#
#    with col4:
#        join1 = st.radio(
#            "–í—ã–±–æ—Ä –ø–µ—Ä–≤–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è Join üëâ",
#            (data1.name, data2.name, data3.name))

#        if join1 == data1.name:
#            join1_df = data1.df
#        if join1 == data2.name:
#            join1_df = data2.df
#        if join1 == data3.name:
#            join1_df = data3.df

#    with col5:
#        join2 = st.radio(
#            "–í—ã–±–æ—Ä –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è Join üëâ",
#            (data1.name, data2.name, data3.name))

#        if join2 == data1.name:
#            join2_df = data1.df
#        if join2 == data2.name:
#            join2_df = data2.df
#        if join2 == data3.name:
#            join2_df = data3.df

#    with col6:
#        join_type = st.radio(
#            "–í—ã–±–æ—Ä —Ç–∏–ø–∞ Join üëâ",
#            ("left", "right", "inner","outer"))
#        join_mark = ("–ü–æ–ø—Ä–æ–±—É–µ–º " + join_type + "join:")

#    with col7:
#        if join_df_2 == "df_ads":
#            join_col_list = list_ads
#        if join_df_2 == "df_leads":
#            join_col_list = list_leads
#        if join_df_2 == "df_purchases":
#            join_col_list = list_purchases
    
#        join_col = st.radio(
#            "–í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è Join üëâ",
#            (join_col_list))

#    st.text("Join columns must have the same name")
#    if st.button('Lets JOIN that!'):
#        st.markdown(join_mark, unsafe_allow_html = True)
#        joined_df = pd.merge(join_df_1_, join_df_2_, how = join_type, on = join_col)
#        st.dataframe(joined_df)

#        buffer = io.StringIO()
#        joined_df.info(buf = buffer)
#        joined_df_info = buffer.getvalue()
#        st.text(joined_df_info)

#        df_to_download = joined_df.to_csv()
#        st.download_button(label='üì• –°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –≤–µ–¥–æ–º–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ .CSV', data = df_to_download, file_name = "Joined dataframe" + ".csv")

#with st.expander("Dataset Filter"):
#    if st.button('if Joined_df sucsessful'):
#        filter = joined_df


#    uploaded_filter = st.file_uploader("Area to filter df")
#    if uploaded_filter is not None:
#        st.write("Filename: ", uploaded_filter.name)
#        df_filter = pd.read_csv(uploaded_filter)
#        st.markdown("""<h5 style='text-align: center;'>–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:</h5>""", unsafe_allow_html = True)
#        st.dataframe(df_filter)

#        cols_filter = df_filter.columns
#        list_filter = []
#        for i in range(len(cols_filter)):
#            list_filter.append(cols_filter[i])

#        st.markdown("""<h5 style='text-align: center;'>–§–∏–ª—å—Ç—Ä –ø–æ–¥ –∑–∞–¥–∞–Ω–∏–µ:</h5>""", unsafe_allow_html = True)
#        #df_filter[(df_filter.m_purchase_amount > 0) & (df_filter.hp > 80)]
#        df_filter2 = df_filter[(df_filter.m_purchase_amount > 0)]

#        df_filter2['datelag'] = 0
#        st.text("added datelag")
#        df_filter2["purchase_created_at"] = pd.to_datetime(df_filter2["purchase_created_at"])
#        df_filter2["lead_created_at"] = pd.to_datetime(df_filter2["lead_created_at"])
#        df_filter2['datelag'] = (df_filter2['purchase_created_at'] - df_filter2['lead_created_at'])
#        df_filter2["datelag"] = df_filter2["datelag"].dt.days
#        st.dataframe(df_filter2)