from asyncore import write
import streamlit as st
from gsheetsdb import connect
import pandas as pd
import io
import requests
from io import StringIO

conn = connect()
@st.cache(ttl=1200)
def run_query(query):
    rows = conn.execute(query, headers=1)
    return rows

st.markdown("<h2 style='text-align: center;'>Datalyzer</h2>", unsafe_allow_html=True)

headcol1, headcol2 = st.columns(2)

with headcol1:
    header_tasklink = """[<h5 style='text-align: center;'>Test task</h5>](https://xoservices.notion.site/1872d331265946a0ae2c5c9069189fd7)"""
    st.markdown(header_tasklink, unsafe_allow_html=True)

with headcol2:
    header_repolink = """[<h5 style='text-align: center;'>Github repo this project</h5>](https://github.com/newer0ne/presentation/blob/main/presentation.py)"""
    st.markdown(header_repolink, unsafe_allow_html=True)

class Dataset:
    def __init__(self) -> None:
        self.name = []  
        self.df = []
        self.listcols = []
        self.link = []
        self.up = None
    
    def Open(self, index):
        self.df = pd.read_csv(index)
        self.name = index
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        self.df['dupl'] = self.df.duplicated()

    def linkup(self, index):
        file_id = self.link.split('/')[-2]
        dwn_url = 'https://drive.google.com/uc?export=download&id=' + file_id
        url2 = requests.get(dwn_url).text
        csv_raw = StringIO(url2)
        self.df = pd.read_csv(csv_raw)
        self.name = index
        st.dataframe(self.df)
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        st.text(self.listcols)

    def upload(self, index):
        self.up = st.file_uploader("CSV file upload area, for example " + index)
        if self.up is not None:
            st.write("File name: ", self.up.name)
            self.df = pd.read_csv(self.up)
            st.dataframe(self.df)
            self.name = self.up.name
            for i in range(len(self.df.columns)):
                self.listcols.append(self.df.columns[i])
            st.text(self.listcols)

    def DFinfo (self):
        st.write('–û–±–∑–æ—Ä  –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞', self.name, ':')
        st.dataframe(self.df)
        st.write('–°–≤–µ–¥–µ–Ω–∏—è –æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ ', self.name, ':')
        i = self.df
        x = io.StringIO()
        i.info(buf = x)
        i = x.getvalue()
        st.text(i)

    def Unique (self):
        st.markdown("<h4 style='text-align: center;'>–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</h4>", unsafe_allow_html=True)
        st.write('–î–ª—è –∫–æ–ª–æ–Ω–æ–∫ –Ω–∏–∂–µ:')
        x = len(self.listcols)
        unicols = []
        for i in range(x):
            unicols.append(self.name + '_' + self.listcols[i])
        unicols = st.columns(x)
        for ii in range(x):
            unicols[ii].text(self.listcols[ii])
            unicols[ii].write(self.df.iloc[:, ii].unique())

    def renamecol(self, oldname, newname):
        self.df[newname] = self.df[oldname]
        del self.df[oldname]
        self.listcols.remove(oldname)
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        st.text(self.listcols)
        st.dataframe(self.df)

opt_desc = ["–û—Ç–∫—Ä—ã—Ç—å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "–û—Ç–∫—Ä—ã—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–µ", "–ó–∞–≥—Ä—É–∑–∏—Ç—å –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö"]

load_option = st.radio(
    "–í—ã–±–æ—Ä —Å–ø–æ—Å–æ–±–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:",
    (opt_desc))

name_list = ["ads.csv", "leads.csv", "purchases.csv"]
data1 = Dataset()
data1.link = st.secrets["ads"]
data2 = Dataset()
data2.link = st.secrets["leads"]
data3 = Dataset()
data3.link = st.secrets["purchases"]
data4 = Dataset()

st.markdown("<h4 style='text-align: center;'>–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</h4>", unsafe_allow_html=True)

if load_option == opt_desc[0]:    
    tab_open1, tab_open2, tab_open3 = st.tabs(name_list)

    with tab_open1:

        data1.Open(name_list[0])

        st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å—É–¥—è –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É,
        –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–∞–ª–∞–º–µ ('d_utm_medium' = 'cpc') 
        –≤ —è–Ω–¥–µ–∫—Å.–¥–∏—Ä–µ–∫—Ç ('d_utm_source' = 'yandex').""")
        data1.DFinfo()
        data1.Unique()

        st.write("""–ö–æ–ª–æ–Ω–∫–∏ 'created_at', 'd_utm_source' –∏ 'd_utm_medium' –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è –¥–ª—è
        –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—è–Ω–∏—è c —Ç–∞–±–ª–∏—Ü–µ–π leads.csv –∫–∞–∫ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã.""")
        st.write("""–ö–æ–ª–æ–Ω–∫–∏ 'd_ad_account_id' –∏ 'd_utm_term' –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        –Ω–µ –Ω–µ—Å—É—Ç —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–¥–∞–ª—è—é—Ç—Å—è, –ø–æ—Å–∫–æ–ª—å–∫—É 'd_ad_account_id' –∏–º–µ–µ—Ç 
        —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ 'xo-for-client-ya', –∞ 'd_utm_term' –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç–æ–π.""")
        st.write("""–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ 'm_clicks' –∏ 'm_cost' –≤ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ,
        –∞ —Ç–∞–∫-–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ 'm_clicks' –±–æ–ª—å—à–µ –Ω—É–ª—è.""")

        data1.df = data1.df.drop(columns = ['d_ad_account_id', 'd_utm_term'])
        data1.df['m_cost'] = data1.df['m_cost'].astype(int)
        data1.df['m_clicks'] = data1.df['m_clicks'].astype(int)
        data1.df = data1.df[data1.df['m_clicks'] > 0]

        st.markdown("<h4 style='text-align: center;'>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π</h4>", unsafe_allow_html=True)
        data1.DFinfo()

        st.markdown("<h4 style='text-align: center;'>–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤</h4>", unsafe_allow_html=True)
        data1.df['Duplicated'] = data1.df.duplicated()
        st.dataframe(data1.df)

    with tab_open2:

        data2.Open(name_list[1])

        st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å—É–¥—è –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É,
        –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –∑–∞—è–≤–∫–∞–º –Ω–∞ —Å–∞–π—Ç–µ.""")
        data2.DFinfo()
        data2.Unique()

        st.write("""–ö–æ–ª–æ–Ω–∫–∏ 'lead_created_at', 'd_lead_utm_source' –∏ 
        'd_lead_utm_medium' –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—è–Ω–∏—è —Å —Ç–∞–±–ª–∏—Ü–µ–π 
        ads.csv –∞ —Ç–∞–∫–∂–µ –∫–æ–ª–æ–Ω–∫–∞ 'client_id' –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—è–Ω–∏—è —Å —Ç–∞–±–ª–∏—Ü–µ–π
        purchases.csv –∫–∞–∫ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏.""")
        st.write("""49 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç—Ä–∞—Ñ–∏–∫–∞ –≤ –∫–æ–ª–æ–Ω–∫–µ 'd_lead_utm_source'
        –≥–æ–≤–æ—Ä–∏—Ç –æ –∞–∫—Ç–∏–≤–Ω–æ —Ä–µ–∫–ª–∞–º–∏—Ä—É–µ–º–æ–º —Å–µ—Ä–≤–∏—Å–µ, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç—Å—è
        –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–∞—Ñ–∏–∫–æ–º. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –∫–∞–∫
        –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é —Ä–µ–∫–ª–∞–º—É, —Ç–∞–∫ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏, –∏–Ω—Ñ–æ–±–æ—Ç—ã, –±–∞–Ω–Ω–µ—Ä—ã –∏ –ø—Ä.""")

        st.write("""–û—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–µ 'd_lead_utm_source' –ø–æ
        –∏—Å—Ç–æ—á–Ω–∏–∫—É 'yandex' –∏ 'd_lead_utm_medium' –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É 'cpc'""")
        data2.df = data2.df[(data2.df['d_lead_utm_source'] == 'yandex') & (data2.df['d_lead_utm_medium'] == 'cpc')]

        st.write("""–ü—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫—É 'client_id' –∫ —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–Ω—ã—Ö str
        –∏ —É–±–µ—Ä–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏–∏ —è—á–µ–π–∫–∞–º–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö 'client_id' –∏ 
        'd_lead_utm_content', –ø–æ—Å–ª–µ —á–µ–≥–æ —É–¥–∞–ª–∏–º 'd_lead_utm_content',
        —Ç.–∫. –Ω–∞ –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏–∫—É –∫–æ–ª–æ–Ω–∫–∞ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç.""")
        data2.df['client_id'] = data2.df['client_id'].astype(str)
        data2.df = data2.df[(data2.df['client_id'] != 'nan')]
        data2.df = data2.df[(data2.df['d_lead_utm_content'].notnull())]
        data2.df = data2.df.drop(columns = ['d_lead_utm_term'])

        st.markdown("<h4 style='text-align: center;'>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π</h4>", unsafe_allow_html=True)
        data2.DFinfo()

        st.markdown("<h4 style='text-align: center;'>–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤</h4>", unsafe_allow_html=True)
        data2.df['Duplicated'] = data2.df.duplicated()
        st.dataframe(data2.df)

    with tab_open3:

        data3.Open(name_list[2])

        st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å—É–¥—è –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É,
        –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –æ–ø–ª–∞—Ç–∞–º –∑–∞—è–≤–æ–∫ –Ω–∞ —Å–∞–π—Ç–µ.""")
        data3.DFinfo()
        data3.Unique()

        data3.df['client_id'] = data3.df['client_id'].astype(str)
        data3.df = data3.df[(data3.df['client_id'] != 'nan')]
        data3.df['m_purchase_amount'] = data3.df['m_purchase_amount'].astype(int)
        data3.df = data3.df[(data3.df['m_purchase_amount'] > 0)]
        data3.DFinfo()

        st.markdown("<h4 style='text-align: center;'>–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤</h4>", unsafe_allow_html=True)
        data3.df['Duplicated'] = data3.df.duplicated()
        st.dataframe(data3.df)

elif load_option == opt_desc[1]:
    tab_load1, tab_load2, tab_load3 = st.tabs(name_list)    
    with tab_load1:
        data1.linkup(name_list[0])    
    with tab_load2:
        data2.linkup(name_list[1])        
    with tab_load3:
        data3.linkup(name_list[2])

elif load_option == opt_desc[2]:
    tab_up1, tab_up2, tab_up3 = st.tabs(name_list)
    with tab_up1:
            data1.upload(name_list[0])
    with tab_up2:
            data2.upload(name_list[1])
    with tab_up3:
            data3.upload(name_list[2])

if (data1.df is None) or (data2.df is None) or (data3.df is None):
    st.error('This is an error', icon="üö®")
    pause()

st.markdown("<h4 style='text-align: center;'>–°–ª–∏—è–Ω–∏–µ —Ç–∞–±–ª–∏—Ü</h4>", unsafe_allow_html=True)

m23 = pd.merge(data2.df, data3.df, how = 'left', on = 'client_id')

st.write("–§–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö astype(int), 'utm_source' == 'yandex' –∏ 'm_purchase_amount'] > 0:")

m23f = m23[(m23['utm_source'] == 'yandex') & (m23['m_purchase_amount'] > 0)]
m23f['m_purchase_amount'] = m23f['m_purchase_amount'].astype(int)
m23f['utm_content'] = m23f['utm_content'].astype(int)
m23f['utm_campaign'] = m23f['utm_campaign'].astype(int)

st.write('')
st.dataframe(m23f)

st.write("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö merget tables –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")

buffer4 = io.StringIO()
m23i = m23f
m23i.info(buf = buffer4)
m23i = buffer4.getvalue()
st.text(m23i)

st.write("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è merge ads + leads_purchase:")

data1.df.astype({'m_clicks': 'int'})
buffer5 = io.StringIO()
data1.dfi = data1.df
data1.dfi.info(buf = buffer5)
data1.dfi = buffer5.getvalue()
st.text(data1.dfi)

m123 = pd.merge(data1.df, m23f, how = 'left', on = ['created_at', 'utm_medium','utm_source', 'utm_campaign', 'utm_content'])
#m123 = m123[(m123.m_purchase_amount > 0)]
#m123['m_purchase_amount'] = m123['m_purchase_amount'].astype(int)

st.text("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö merget tables ads_leads_purchase:")

buffer6 = io.StringIO()
m123i = m123
m123i.info(buf = buffer6)
m123i = buffer6.getvalue()
st.text(m123i)

st.text("–£–¥–∞–ª—è–µ–º —Å—Ç–æ–±–µ—Ü 'utm_content', 'purchase_id' –∏ 'client_id':")

m123 = m123.drop(columns = ['utm_content', 'purchase_id', 'client_id'])
st.dataframe(m123)

st.text('–û–ø—Ä–µ–¥–µ–ª–∏–º —Å—Ç—Ä–æ–∫–∏ —Å —Ä–∞–∑–Ω–∏—Ü–µ–π –ø–æ –æ–ø–ª–∞—Ç–∞–º –≤ 15 –¥–Ω–µ–π:')

m123['created_at'] = m123['created_at'].astype(str)
m123['DATE'] = pd.to_datetime(m123['created_at'], infer_datetime_format=True)  
m123['DATE'] = pd.to_datetime(m123['DATE'], format = "%y-%m-%d")
m123['DATE_P'] = pd.to_datetime(m123['purchase_created_at'], infer_datetime_format=True)  
m123['DATE_P'] = pd.to_datetime(m123['DATE_P'], format = "%y-%m-%d")
m123['Difference'] = (m123['DATE_P'] - m123['DATE']).dt.days
m123 = m123.drop(columns = ['DATE', 'DATE_P'])
m123 = m123[(m123.Difference >= 0)]

buffer7 = io.StringIO()
m123ii = m123
m123ii.info(buf = buffer7)
m123ii = buffer7.getvalue()

st.text('–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∞–±–ª–∏—Ü—ã –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –¥–∞—Ç—ã')
st.dataframe(m123)
st.text(m123ii)

#m123['–î–∞—Ç–∞_–æ–ø–ª–∞—Ç—ã'] = datetime.strptime(m123['purchase_created_at'], '%m-%d-%Y').date()
#m123['Difference'] = (m123['–î–∞—Ç–∞'] - m123['–î–∞—Ç–∞_–æ–ø–ª–∞—Ç—ã']).dt.days
#st.dataframe(m123)


#buffer = io.StringIO()
#joined123.info(buf = buffer)
#joined_df_info = buffer.getvalue()
#st.text(joined_df_info)

#df_to_download = joined123.to_csv()
#st.download_button(label='üì• Download .CSV', data = df_to_download, file_name = "Joined dataframe" + ".csv")



#st.write("Dataframe Renamer")
#ren1, ren2, ren3 = st.columns(3)

#with ren1:
#    data4.name = st.radio(
#        "Dataframe selection to rename üëâ",
#        (data1.name, data2.name, data3.name))
#    st.write("Choosed dataframe: " + data4.name)

#with ren2:
#    if data4.name == data1.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data1.listcols))
#        st.write("Choosed column: " + ren_col)
#    elif data4.name == data2.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data2.listcols))
#        st.write("Choosed column: " + ren_col)
#    elif data4.name == data3.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data3.listcols))
#        st.write("Choosed column: " + ren_col)

#with ren3:
#    newcolname = st.text_input('New column name', ren_col)
#    st.write("New name for the selected column: " + newcolname)

#if st.button('Lets change it'):
#    if data4.name == name_list[0]:
#        data1.renamecol(ren_col, newcolname)
#    elif data4.name == name_list[1]:
#        data2.renamecol(ren_col, newcolname)
#    elif data4.name == name_list[2]:
#        data3.renamecol(ren_col, newcolname)


#with st.expander("Dataset Joiner"):
#    col4, col5, col6, col7 = st.columns(4)
#
#    with col4:
#        join1 = st.radio(
#            "–í—ã–±–æ—Ä –ø–µ—Ä–≤–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è Join üëâ",
#            (data1.name, data2.name, data3.name))

#        if join1 == data1.name:
#            join1_df = data1.df
#        if join1 == data2.name:
#            join1_df = data2.df
#        if join1 == data3.name:
#            join1_df = data3.df

#    with col5:
#        join2 = st.radio(
#            "–í—ã–±–æ—Ä –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è Join üëâ",
#            (data1.name, data2.name, data3.name))

#        if join2 == data1.name:
#            join2_df = data1.df
#        if join2 == data2.name:
#            join2_df = data2.df
#        if join2 == data3.name:
#            join2_df = data3.df

#    with col6:
#        join_type = st.radio(
#            "–í—ã–±–æ—Ä —Ç–∏–ø–∞ Join üëâ",
#            ("left", "right", "inner","outer"))
#        join_mark = ("–ü–æ–ø—Ä–æ–±—É–µ–º " + join_type + "join:")

#    with col7:
#        if join_df_2 == "df_ads":
#            join_col_list = list_ads
#        if join_df_2 == "df_leads":
#            join_col_list = list_leads
#        if join_df_2 == "df_purchases":
#            join_col_list = list_purchases
    
#        join_col = st.radio(
#            "–í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è Join üëâ",
#            (join_col_list))

#    st.text("Join columns must have the same name")
#    if st.button('Lets JOIN that!'):
#        st.markdown(join_mark, unsafe_allow_html = True)
#        joined_df = pd.merge(join_df_1_, join_df_2_, how = join_type, on = join_col)
#        st.dataframe(joined_df)

#        buffer = io.StringIO()
#        joined_df.info(buf = buffer)
#        joined_df_info = buffer.getvalue()
#        st.text(joined_df_info)

#        df_to_download = joined_df.to_csv()
#        st.download_button(label='üì• –°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –≤–µ–¥–æ–º–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ .CSV', data = df_to_download, file_name = "Joined dataframe" + ".csv")

#with st.expander("Dataset Filter"):
#    if st.button('if Joined_df sucsessful'):
#        filter = joined_df


#    uploaded_filter = st.file_uploader("Area to filter df")
#    if uploaded_filter is not None:
#        st.write("Filename: ", uploaded_filter.name)
#        df_filter = pd.read_csv(uploaded_filter)
#        st.markdown("""<h5 style='text-align: center;'>–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:</h5>""", unsafe_allow_html = True)
#        st.dataframe(df_filter)

#        cols_filter = df_filter.columns
#        list_filter = []
#        for i in range(len(cols_filter)):
#            list_filter.append(cols_filter[i])

#        st.markdown("""<h5 style='text-align: center;'>–§–∏–ª—å—Ç—Ä –ø–æ–¥ –∑–∞–¥–∞–Ω–∏–µ:</h5>""", unsafe_allow_html = True)
#        #df_filter[(df_filter.m_purchase_amount > 0) & (df_filter.hp > 80)]
#        df_filter2 = df_filter[(df_filter.m_purchase_amount > 0)]

#        df_filter2['datelag'] = 0
#        st.text("added datelag")
#        df_filter2["purchase_created_at"] = pd.to_datetime(df_filter2["purchase_created_at"])
#        df_filter2["lead_created_at"] = pd.to_datetime(df_filter2["lead_created_at"])
#        df_filter2['datelag'] = (df_filter2['purchase_created_at'] - df_filter2['lead_created_at'])
#        df_filter2["datelag"] = df_filter2["datelag"].dt.days
#        st.dataframe(df_filter2)