from asyncore import write
import streamlit as st
from gsheetsdb import connect
import pandas as pd
import io
import requests
from io import StringIO

conn = connect()
@st.cache(ttl=1200)
def run_query(query):
    rows = conn.execute(query, headers=1)
    return rows

class Dataset:
    def __init__(self) -> None:
        self.name = []  
        self.df = []
        self.dfd = []
        self.listcols = []
        self.link = []
        self.up = None
    
    def Open(self, index):
        self.df = pd.read_csv(index)
        self.name = index
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        self.df.drop_duplicates()

    def linkup(self, index):
        file_id = self.link.split('/')[-2]
        dwn_url = 'https://drive.google.com/uc?export=download&id=' + file_id
        url2 = requests.get(dwn_url).text
        csv_raw = StringIO(url2)
        self.df = pd.read_csv(csv_raw)
        self.name = index
        st.dataframe(self.df)
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        st.text(self.listcols)

    def upload(self, index):
        self.up = st.file_uploader("CSV file upload area, for example " + index)
        if self.up is not None:
            st.write("File name: ", self.up.name)
            self.df = pd.read_csv(self.up)
            st.dataframe(self.df)
            self.name = self.up.name
            for i in range(len(self.df.columns)):
                self.listcols.append(self.df.columns[i])
            st.text(self.listcols)

    def Lcols(self):
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])

    def DFinfo (self):
        st.write('–û–±–∑–æ—Ä  –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞', self.name, ':')
        st.dataframe(self.df)
        st.write('–°–≤–µ–¥–µ–Ω–∏—è –æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ ', self.name, ':')
        i = self.df
        x = io.StringIO()
        i.info(buf = x)
        i = x.getvalue()
        st.text(i)

    def Unique (self):
        st.markdown("<h4 style='text-align: center;'>–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è</h4>", unsafe_allow_html=True)
        st.write('–î–ª—è –∫–æ–ª–æ–Ω–æ–∫ –Ω–∏–∂–µ:')
        x = len(self.listcols)
        unicols = []
        for i in range(x):
            unicols.append(self.name + '_' + self.listcols[i])
        unicols = st.columns(x)
        for ii in range(x):
            unicols[ii].text(self.listcols[ii])
            unicols[ii].text(len(self.df.iloc[:, ii].unique()))
            unicols[ii].write(self.df.iloc[:, ii].unique())

    def renamecol(self, oldname, newname):
        self.df[newname] = self.df[oldname]
        del self.df[oldname]
        self.listcols.remove(oldname)
        for i in range(len(self.df.columns)):
            self.listcols.append(self.df.columns[i])
        st.text(self.listcols)
        st.dataframe(self.df)

st.markdown("<h2 style='text-align: center;'>Datalyzer</h2>", unsafe_allow_html=True)

headcol1, headcol2 = st.columns(2)

with headcol1:
    header_tasklink = """[<h5 style='text-align: center;'>Test task</h5>](https://xoservices.notion.site/1872d331265946a0ae2c5c9069189fd7)"""
    st.markdown(header_tasklink, unsafe_allow_html=True)

with headcol2:
    header_repolink = """[<h5 style='text-align: center;'>Github repo this project</h5>](https://github.com/newer0ne/presentation/blob/main/presentation.py)"""
    st.markdown(header_repolink, unsafe_allow_html=True)

name_list = ["ads.csv", "leads.csv", "purchases.csv"]
opt_desc = ["–û—Ç–∫—Ä—ã—Ç—å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö", "–û—Ç–∫—Ä—ã—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Å—ã–ª–∫–µ", "–ó–∞–≥—Ä—É–∑–∏—Ç—å –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö"]

data1 = Dataset()
data1.link = st.secrets["ads"]
data2 = Dataset()
data2.link = st.secrets["leads"]
data3 = Dataset()
data3.link = st.secrets["purchases"]
data23 = Dataset()
data123 = Dataset()

with st.expander('–í—ã–±–æ—Ä —Å–ø–æ—Å–æ–±–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö'):
    
    load_option = st.radio(
        "–í—ã–±–æ—Ä —Å–ø–æ—Å–æ–±–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:",
        (opt_desc))

with st.expander('–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'):

    if load_option == opt_desc[0]:

        st.markdown("<h4 style='text-align: center;'>–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ</h4>", unsafe_allow_html=True)

        tab_open1, tab_open2, tab_open3 = st.tabs(name_list)

        with tab_open1:

            data1.Open(name_list[0])

            st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç 
            —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–∞–ª–∞–º–µ ('d_utm_medium' = 'cpc') 
            –≤ —è–Ω–¥–µ–∫—Å.–¥–∏—Ä–µ–∫—Ç ('d_utm_source' = 'yandex').""")
            data1.DFinfo()
            data1.Unique()

            st.write("""–ö–æ–ª–æ–Ω–∫–∏ 'created_at', 'd_utm_source' –∏ 'd_utm_medium' –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è –¥–ª—è
            –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—è–Ω–∏—è c —Ç–∞–±–ª–∏—Ü–µ–π leads.csv –∫–∞–∫ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã.""")
            
            st.write("""–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ 'm_clicks' –∏ 'm_cost' –≤ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ.""")
            data1.df['m_cost'] = data1.df['m_cost'].astype(int)
            data1.df['m_clicks'] = data1.df['m_clicks'].astype(int)

            data1.df.rename(columns = {
                'd_ad_account_id':'account_id',
                'd_utm_source':'source',
                'd_utm_medium':'medium',
                'd_utm_campaign':'campaign',
                'd_utm_content':'content',
                'd_utm_term':'term',
                'm_clicks':'clicks',
                'm_cost':'cost'
                }, inplace = True)

            st.markdown("<h4 style='text-align: center;'>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π</h4>", unsafe_allow_html=True)
            data1.DFinfo()

        with tab_open2:

            data2.Open(name_list[1])

            st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç
            —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –∑–∞—è–≤–∫–∞–º –Ω–∞ —Å–∞–π—Ç–µ. –ü–æ —Å—É—Ç–∏ —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è
            —Å –∫–æ—Ç–æ—Ä–æ–π –±—É–¥—É—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å—Å—è –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª—è–∏–Ω–∏—è.""")
            data2.DFinfo()
            data2.Unique()

            st.write("""–ö–æ–ª–æ–Ω–∫–∏ 'lead_created_at', 'd_lead_utm_source' –∏ 
            'd_lead_utm_medium' –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—è–Ω–∏—è —Å —Ç–∞–±–ª–∏—Ü–µ–π 
            ads.csv –∞ —Ç–∞–∫–∂–µ –∫–æ–ª–æ–Ω–∫–∞ 'client_id' –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—è–Ω–∏—è —Å —Ç–∞–±–ª–∏—Ü–µ–π
            purchases.csv –∫–∞–∫ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏.""")

            st.write("""–ü—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫—É 'client_id' –∫ —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–Ω—ã—Ö str
            –∏ —É–±–µ—Ä–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏–∏ —è—á–µ–π–∫–∞–º–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö 'client_id' –∏ 
            'd_lead_utm_content', –ø–æ—Å–ª–µ —á–µ–≥–æ —É–¥–∞–ª–∏–º 'd_lead_utm_content',
            —Ç.–∫. –Ω–∞ –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏–∫—É –∫–æ–ª–æ–Ω–∫–∞ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç.""")
            data2.df['client_id'] = data2.df['client_id'].astype(str)

            data2.df.rename(columns = {
                'lead_created_at': 'created_at',
                'd_lead_utm_source':'source',
                'd_lead_utm_medium':'medium',
                'd_lead_utm_campaign':'campaign',
                'd_lead_utm_content':'content',
                'd_lead_utm_term':'term',
                }, inplace = True)

            st.markdown("<h4 style='text-align: center;'>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π</h4>", unsafe_allow_html=True)
            data2.DFinfo()

        with tab_open3:

            data3.Open(name_list[2])

            st.write("""–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç
            —Å—Ç–∞—Ç–∏—Å—Ç–∫—É –ø–æ –æ–ø–ª–∞—Ç–∞–º.""")
            data3.DFinfo()
            data3.Unique()

            st.write("""–ü—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫—É 'client_id' –∫ —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–Ω—ã—Ö str
            –∏ —É–±–µ—Ä–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏–∏ —è—á–µ–π–∫–∞–º–∏ –≤ –∫–æ–ª–æ–Ω–∫–µ 'client_id'.""")
            data3.df['client_id'] = data3.df['client_id'].astype(str)

            st.write("""–ü—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫—É 'm_purchase_amount' –∫ —Ñ–æ—Ä–º–∞—Ç—É –¥–∞–Ω–Ω—ã—Ö int
            –∏ –æ—Ç–æ–±—Ä–∞–∑–º –≤ –Ω–µ–π —Å—Ç—Ä–æ–∫–∏ —Å —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –±–æ–ª—å—à–µ –Ω—É–ª—è.""")
            data3.df['m_purchase_amount'] = data3.df['m_purchase_amount'].astype(int)
            data3.DFinfo()

    elif load_option == opt_desc[1]:
        tab_load1, tab_load2, tab_load3 = st.tabs(name_list)
        with tab_load1:
            data1.linkup(name_list[0])    
        with tab_load2:
            data2.linkup(name_list[1])
        with tab_load3:
            data3.linkup(name_list[2])

    elif load_option == opt_desc[2]:
        tab_up1, tab_up2, tab_up3 = st.tabs(name_list)
        with tab_up1:
                data1.upload(name_list[0])
        with tab_up2:
                data2.upload(name_list[1])
        with tab_up3:
                data3.upload(name_list[2])

    if (data1.df is None) or (data2.df is None) or (data3.df is None):
        st.error('This is an error', icon="üö®")

with st.expander('–°–ª–∏—è–Ω–∏–µ —Ç–∞–±–ª–∏—Ü leads + purchase'):
    
    st.markdown("<h4 style='text-align: center;'>–°–ª–∏—è–Ω–∏–µ —Ç–∞–±–ª–∏—Ü leads –∏ purchase</h4>", unsafe_allow_html=True)
    data23.df = pd.merge(data2.df, data3.df, how = 'left', on = 'client_id')
    data23.Lcols()
    data23.name = data2.name + ' & ' + data3.name
    data23.DFinfo()

    st.write("""–ü—Ä–æ–≤–µ–¥–µ–º —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–ª–æ–Ω–∫–µ 'd_lead_utm_source'
    –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Ç—Ä–∞—Ñ–∏–∫–∞ —Å 'yandex'.""")
    st.write("""–ü—Ä–∏–≤–µ–¥–µ–º –∫–æ–ª–æ–Ω–∫–∏ 'd_lead_utm_content', 
    'd_lead_utm_campaign' –∫ —Ñ–æ–º–∞—Ç—É –¥–∞–Ω–Ω—ã—Ö int.""")

    data23.DFinfo()
    data23.Unique()
    st.write("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞—è–≤–æ–∫", len(data23.df['lead_id'].unique()), 
    "–±–æ–ª—å—à–µ, —á–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ ", len(data23.df['client_id'].unique()), ",",
    "–ø–æ—ç—Ç–æ–º—É —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–ª—è –∫–∞–∫–∏—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –±—ã–ª–æ –∑–∞–≤–µ–¥–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞—è–≤–æ–∫.")

with st.expander("–°–ª–∏—è–Ω–∏–µ —Ç–∞–±–ª–∏—Ü ads + leads_purchase"):

    st.markdown("<h4 style='text-align: center;'>–°–ª–∏—è–Ω–∏–µ —Ç–∞–±–ª–∏—Ü ads + leads_purchase</h4>", unsafe_allow_html=True)
    data1.df = data1.df.astype({'campaign': 'str', 'content': 'str', 'term': 'str', 'clicks': 'str'})
    data1.DFinfo()
    

    data123.df = pd.merge(data1.df, data23.df, on = ['created_at', 'medium','source', 'campaign', 'content', 'term'], how = 'outer')
    data123.Lcols()
    data123.name = data1.name + ' & ' + data23.name
    data123.DFinfo()

with st.expander('–ê—Ç—Ä–∏–±—É—Ü–∏—è –õ–∏–¥-–ü—Ä–æ–¥–∞–∂–∞'):

    st.markdown("<h4 style='text-align: center;'>–ê—Ç—Ä–∏–±—É—Ü–∏—è –õ–∏–¥-–ü—Ä–æ–¥–∞–∂–∞</h4>", unsafe_allow_html=True)

    grouped = data123.df.groupby('client_id')
    #grouped.get_group('client_id')
    st.dataframe(grouped)


    #data23.df['dupl'] = data23.df.duplicated(subset=['client_id'])
    #data23.dfd = data23.df[data23.df['dupl'] == True]
    #st.write("–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –∫–æ–ª–æ–Ω–∫–µ 'client_id' = ", len(data23.dfd['dupl'] == True), ".")
    #st.dataframe(data23.dfd)


#    st.text('–û–ø—Ä–µ–¥–µ–ª–∏–º —Å—Ç—Ä–æ–∫–∏ —Å —Ä–∞–∑–Ω–∏—Ü–µ–π –ø–æ –æ–ø–ª–∞—Ç–∞–º –≤ 15 –¥–Ω–µ–π:')

#data123.df['created_at'] = data123.df['created_at'].astype(str)
#data123.df['DATE'] = pd.to_datetime(data123.df['created_at'], infer_datetime_format=True)  
#data123.df['DATE'] = pd.to_datetime(data123.df['DATE'], format = "%y-%m-%d")
#data123.df['DATE_P'] = pd.to_datetime(data123.df['purchase_created_at'], infer_datetime_format=True)  
#data123.df['DATE_P'] = pd.to_datetime(data123.df['DATE_P'], format = "%y-%m-%d")
#data123.df['Difference'] = (data123.df['DATE_P'] - data123.df['DATE']).dt.days
#data123.df = data123.df.drop(columns = ['DATE', 'DATE_P'])
#data123.df = data123.df[(data123.df['Difference'] >= 0)]
#data123.DFinfo()

#st.text('–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∞–±–ª–∏—Ü—ã –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –¥–∞—Ç—ã')
#m123['–î–∞—Ç–∞_–æ–ø–ª–∞—Ç—ã'] = datetime.strptime(m123['purchase_created_at'], '%m-%d-%Y').date()
#m123['Difference'] = (m123['–î–∞—Ç–∞'] - m123['–î–∞—Ç–∞_–æ–ø–ª–∞—Ç—ã']).dt.days
#st.dataframe(m123)


#buffer = io.StringIO()
#joined123.info(buf = buffer)
#joined_df_info = buffer.getvalue()
#st.text(joined_df_info)

#df_to_download = joined123.to_csv()
#st.download_button(label='üì• Download .CSV', data = df_to_download, file_name = "Joined dataframe" + ".csv")



#st.write("Dataframe Renamer")
#ren1, ren2, ren3 = st.columns(3)

#with ren1:
#    data4.name = st.radio(
#        "Dataframe selection to rename üëâ",
#        (data1.name, data2.name, data3.name))
#    st.write("Choosed dataframe: " + data4.name)

#with ren2:
#    if data4.name == data1.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data1.listcols))
#        st.write("Choosed column: " + ren_col)
#    elif data4.name == data2.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data2.listcols))
#        st.write("Choosed column: " + ren_col)
#    elif data4.name == data3.name:
#        ren_col = st.radio(
#            "Columns selection üëâ",
#            (data3.listcols))
#        st.write("Choosed column: " + ren_col)

#with ren3:
#    newcolname = st.text_input('New column name', ren_col)
#    st.write("New name for the selected column: " + newcolname)

#if st.button('Lets change it'):
#    if data4.name == name_list[0]:
#        data1.renamecol(ren_col, newcolname)
#    elif data4.name == name_list[1]:
#        data2.renamecol(ren_col, newcolname)
#    elif data4.name == name_list[2]:
#        data3.renamecol(ren_col, newcolname)


#with st.expander("Dataset Joiner"):
#    col4, col5, col6, col7 = st.columns(4)
#
#    with col4:
#        join1 = st.radio(
#            "–í—ã–±–æ—Ä –ø–µ—Ä–≤–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è Join üëâ",
#            (data1.name, data2.name, data3.name))

#        if join1 == data1.name:
#            join1_df = data1.df
#        if join1 == data2.name:
#            join1_df = data2.df
#        if join1 == data3.name:
#            join1_df = data3.df

#    with col5:
#        join2 = st.radio(
#            "–í—ã–±–æ—Ä –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è Join üëâ",
#            (data1.name, data2.name, data3.name))

#        if join2 == data1.name:
#            join2_df = data1.df
#        if join2 == data2.name:
#            join2_df = data2.df
#        if join2 == data3.name:
#            join2_df = data3.df

#    with col6:
#        join_type = st.radio(
#            "–í—ã–±–æ—Ä —Ç–∏–ø–∞ Join üëâ",
#            ("left", "right", "inner","outer"))
#        join_mark = ("–ü–æ–ø—Ä–æ–±—É–µ–º " + join_type + "join:")

#    with col7:
#        if join_df_2 == "df_ads":
#            join_col_list = list_ads
#        if join_df_2 == "df_leads":
#            join_col_list = list_leads
#        if join_df_2 == "df_purchases":
#            join_col_list = list_purchases
    
#        join_col = st.radio(
#            "–í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è Join üëâ",
#            (join_col_list))

#    st.text("Join columns must have the same name")
#    if st.button('Lets JOIN that!'):
#        st.markdown(join_mark, unsafe_allow_html = True)
#        joined_df = pd.merge(join_df_1_, join_df_2_, how = join_type, on = join_col)
#        st.dataframe(joined_df)

#        buffer = io.StringIO()
#        joined_df.info(buf = buffer)
#        joined_df_info = buffer.getvalue()
#        st.text(joined_df_info)

#        df_to_download = joined_df.to_csv()
#        st.download_button(label='üì• –°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –≤–µ–¥–æ–º–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ .CSV', data = df_to_download, file_name = "Joined dataframe" + ".csv")

#with st.expander("Dataset Filter"):
#    if st.button('if Joined_df sucsessful'):
#        filter = joined_df


#    uploaded_filter = st.file_uploader("Area to filter df")
#    if uploaded_filter is not None:
#        st.write("Filename: ", uploaded_filter.name)
#        df_filter = pd.read_csv(uploaded_filter)
#        st.markdown("""<h5 style='text-align: center;'>–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:</h5>""", unsafe_allow_html = True)
#        st.dataframe(df_filter)

#        cols_filter = df_filter.columns
#        list_filter = []
#        for i in range(len(cols_filter)):
#            list_filter.append(cols_filter[i])

#        st.markdown("""<h5 style='text-align: center;'>–§–∏–ª—å—Ç—Ä –ø–æ–¥ –∑–∞–¥–∞–Ω–∏–µ:</h5>""", unsafe_allow_html = True)
#        #df_filter[(df_filter.m_purchase_amount > 0) & (df_filter.hp > 80)]
#        df_filter2 = df_filter[(df_filter.m_purchase_amount > 0)]

#        df_filter2['datelag'] = 0
#        st.text("added datelag")
#        df_filter2["purchase_created_at"] = pd.to_datetime(df_filter2["purchase_created_at"])
#        df_filter2["lead_created_at"] = pd.to_datetime(df_filter2["lead_created_at"])
#        df_filter2['datelag'] = (df_filter2['purchase_created_at'] - df_filter2['lead_created_at'])
#        df_filter2["datelag"] = df_filter2["datelag"].dt.days
#        st.dataframe(df_filter2)